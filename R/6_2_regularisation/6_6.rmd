---
title: "ISIL_6_6"
output: github_document
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## ISIL_6_6 Ridge Regression and Lasso

### 6.6.1 Best Subset

The regsubsets() function (part of the leaps library) performs best subregsubsets() set selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm()

Load the required librarys for the question.

The model.matrix() function is particularly useful for creating x; not only does it produce a matrix corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy variables.
```{r library, include=FALSE}
library(ISLR)
# [-1] to remove the intercept
Hitters <- na.omit(Hitters)
x <- model.matrix(Salary~., Hitters)[,-1]
y <- Hitters$Salary
```

The glmnet() function standardizes the variables so that they are on the same scale
```{r rr_1}
library(glmnet)
# grid of lambda values from 10^10 to 10^-2
grid <- 10^seq(10, -2, length=100)
ridge_reg_model <- glmnet(x, y, alpha=0, lambda=grid)
dim(coef(ridge_reg_model))
```

Associated with each value of λ is a vector of ridge regression coefficients, stored in a matrix that can be accessed by coef(). 
In this case, it is a 20×100, with 20 rows (one for each predictor, plus an intercept) and 100 columns (one for each value of λ).

Get a bigger value of lambda vs a smaller value and compare.
```{r rr_2}
# get the value of lambda
ridge_reg_model$lambda[50]
coef(ridge_reg_model)[, 50]
sqrt(sum(coef(ridge_reg_model)[-1, 50]^2))
ridge_reg_model$lambda[60]
coef(ridge_reg_model)[, 60]
# l2 norm
sqrt(sum(coef(ridge_reg_model)[-1, 60]^2))
```
We can use the predict() function for a number of purposes. For instance, we can obtain the ridge regression coefficients for a new value of λ, say 50
```{r rr_3}
predict(ridge_reg_model, s=50, type="coefficients")[1:20, ]
```

Next we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using λ = 4. Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument
```{r rr_4}
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
y_test = y[test]

# small lambda
ridge_model = glmnet(x[train, ], y[train], alpha=0, lambda=grid, thresh = 1e-12)
ridge_pred = predict(ridge_model, s=4, newx=x[test, ])
mean((ridge_pred - y_test)^2)
# large lambda
ridge_model = glmnet(x[train, ], y[train], alpha=0, lambda=grid, thresh = 1e-12)
ridge_pred = predict(ridge_model, s=1e10, newx=x[test, ])
mean((ridge_pred - y_test)^2)
# LS regression
ridge_model = glmnet(x[train, ], y[train], alpha=0, lambda=grid, thresh = 1e-12)
ridge_pred = predict(ridge_model, s=0, newx=x[test, ])
mean((ridge_pred - y_test)^2)
```
In this case, small lambda trumphs large lambda and Least Square, and Leas square trumphs large lambda.

Use Cross Validation to choose the Lambda value
```{r rr_5}
set.seed(1)
cv_ridge_model <- cv.glmnet(x[train, ], y[train], alpha=0)
plot(cv_ridge_model)
best_lambda <- cv_ridge_model$lambda.min
best_lambda
ridge_pred = predict(cv_ridge_model, s=best_lambda, newx=x[test, ])
mean((ridge_pred - y_test)^2)
```

Refit the lambda on the full data set
```{r rr_6}
out <- glmnet(x, y, alpha=0)
ridge_coeff = predict(out, type="coefficients", s=best_lambda)[1:20, ]
ridge_coeff
```

### 6.6.2 Lasso

```{r rr_7}
lasso_model <- glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso_model)

set.seed(1)
cv_lasso_model <- cv.glmnet(x[train, ], y[train], alpha=1)
plot(cv_lasso_model)
best_lambda <- cv_lasso_model$lambda.min
best_lambda
ridge_pred = predict(cv_lasso_model, s=best_lambda, newx=x[test, ])
mean((ridge_pred - y_test)^2)

out=glmnet (x, y, alpha=1, lambda=grid)
lasso_coef = predict (out ,type ="coefficients", s=best_lambda )[1:20 ,]
lasso_coef
```
